# Lecture 1
- ![L1.pdf](../assets/L1_1736443730816_0.pdf)
- What is Deep Learning?
	- ((678007b3-effa-477d-a5ff-6f13341d227c))
	- Based on computational models called neural networks
- What are neural networks?
	- $h=\sum_{i=1}^{M}wi.xi$
	- $y=a\left\lbrack h\right\rbrack$
- Minsly and Papert 1969
- Rumelhart, Hinton and williams, 1986 -> Backpropogation
- Deep Learning -> Early 2000s
-
-
- # Lecture 2
- ![L2_Slides.pdf](../assets/L2_Slides_1736444175435_0.pdf)
- Examples of Supervised learning
	- Regression, Multivariate Regression, Binary Classification, Multiclass Classification
- Types of Supervised Learning
	- Regression Problem : Model predicts real values
	- Classification problems: Model predicts discrete values
	- Simplest model => Linear Model => $f\left(x;\theta\right)=\theta0+\theta1.x$ => can only describe relationship as line.
	- ((67800a13-8223-4782-b31d-07f321d2e6e3))
- Piecewise Linear Function
	- can be made with help of activation function
	- Step or ReLU function
	- ((67800ae7-0bf0-4a60-b1f4-366f4273117f))
	- Unique excercise - check pdf
	- ((67800b64-eb73-46b0-8cde-e34830f019e4))
	-
	- What factors influence curves shape?
		- Number of parameters/weights
		- Types of activation functions: ReLU, Sigmoid, tanh
		- Data
	- How many linear regions or segments with D hidden units under ReLU activation?
		- ((67800c05-d458-46cb-bc66-d10e03fd838d))
	- With enough hidden units(linear regions), we can describe any 1D function with arbitrary accuracy
	- Universal approximation theorem
		- ((67800c6a-2978-4d79-b2bd-26cef992021e))
	- A deeper network can represent a given function with fewer parameters compared to wider network
	-
	-
- # Lecture 3
- ![L3_slides.pdf](../assets/L3_slides_1736445139169_0.pdf)
- Writing the basic
- # Lecture 4
-
-