- **Data Mining Notes**: Apriori Principle, Association Rule Mining, and Frequent Itemset Mining
	- **1. Apriori Principle**
		- **Definition**:
			- The **Apriori Principle** states:
				- If an itemset is **frequent**, then all of its subsets must also be frequent.
				- Conversely, if an itemset is **infrequent**, all of its supersets will also be infrequent.
		- **Key Points**:
			- **Support**: The proportion of transactions in which the itemset appears.
				- Formula: `Support = (Number of Transactions Containing Itemset) / (Total Transactions)`
			- The Apriori Principle helps in **pruning** the search space by eliminating candidate itemsets that cannot be frequent.
		- **Example**:
			- If `{A, B}` is infrequent, then `{A, B, C}` and `{A, B, D}` cannot be frequent.
	- **2. Association Rule Mining**
		- **Definition**:
			- Identifies **relationships** between items in a transaction database.
		- **Rule Format**:
			- **Rule**: `X → Y`
				- `X`: Antecedent (if this happens...)
				- `Y`: Consequent (...then that happens).
		- **Metrics**:
			- **Support**: Measures how frequently the rule occurs.
				- Formula: `Support(X → Y) = (Transactions Containing Both X and Y) / (Total Transactions)`
			- **Confidence**: Measures the likelihood of `Y` given `X`.
				- Formula: `Confidence(X → Y) = Support(X ∪ Y) / Support(X)`
			- **Lift**: Measures how much more likely `Y` is given `X` compared to random chance.
				- Formula: `Lift(X → Y) = Confidence(X → Y) / Support(Y)`
		- **Example**:
			- Rule: `{Milk} → {Bread}`
				- Support: 30%
				- Confidence: 70%
				- Lift: 1.2
	- **3. Frequent Itemset Mining**
		- **Definition**:
			- Finds itemsets that occur frequently in a dataset, based on a minimum support threshold.
		- **Steps**:
			- **Candidate Generation**: Generate all possible itemsets of length `k`.
			- **Pruning**: Use the Apriori Principle to eliminate infrequent candidates.
			- **Counting Support**: Calculate support for remaining candidates.
			- **Repeat**: Increase `k` and repeat until no more frequent itemsets can be found.
		- **Algorithms**:
			- **Apriori Algorithm**: Uses the Apriori Principle for efficient frequent itemset mining.
			- **FP-Growth**: Builds a compact **Frequent Pattern Tree** for mining without candidate generation.
		- **Example**:
			- **Dataset**:
				- `T1: Milk, Bread, Butter`
				- `T2: Milk, Bread`
				- `T3: Bread, Butter`
				- `T4: Milk, Butter`
			- **Frequent Itemsets (Min Support = 50%)**:
				- 1-itemsets: `{Milk}`, `{Bread}`, `{Butter}`
				- 2-itemsets: `{Milk, Bread}`, `{Bread, Butter}`
				- 3-itemsets: `{Milk, Bread, Butter}`
	- **Summary**:
		- **Apriori Principle**: Helps reduce computational effort in frequent itemset mining.
		- **Association Rule Mining**: Uncovers hidden relationships using rules.
		- **Frequent Itemset Mining**: Identifies the most common itemsets for generating rules.