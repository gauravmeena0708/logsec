# Lecture 1
collapsed:: true
	- ## Definition
		- Extracting knowledge from the data - pattern, relationships
		- overlaps with databases, ML, CS algo
	- ## Syllabus
		- Association rule Mining
		- Frequency Subgraph mining
		- Searching and Indexing
		- Clustering
		- Random walks and page rank
		- Deep learning for graphs
		- Mining data stream
		- Anomaly Detection
	- ## Structure
		- Homework - 3-4 - 50%
		- Minor                    - 20%
		- Major                    -  20%
		- Quizzes                 - 10%
	- ## HW0
		- Get HPC access, Baadal access, Piazza, github repo
	- ## Matter
		- Association rule mining
			- What is support? - It should have happened in n transactions
			- what is confidence? - Probability of A->B / Probability of A
			- $S=\frac{\left|AUB\right|}{\left|T\right|}$
			- $C=\frac{\left|AUB\right|}{\left|A\right|}$
		- Frequent Itemset mining
			- Any set having S>=s
			- Monotonicity of freq of set
				- if item A has not crossed s, A->B will not be counted
			- for N items $2^{n}-1$ combination of subset can be possible
			-
- # Lecture 2
  collapsed:: true
	- ## Apriori Principle
		- If Itemset is frequent, all its subset must be frequent
	- ## Association Rule mining
		- [[Apriori Principle Association Rule Mining and Frequent Itemset Mining]]
		- TODO Upload the assest and link the correct page here
	- ## Frequent Itemset Mining
		- Naive Approach
			- Generate Candidate - > check support - if frequent accept else throw
			- Total possible subsets - $2^{n}-1$
			- Impractical
		- TODO Prove Fn X Fn will give all subsets of Fn+1
		- TODO while calculating C3, Why discarding the size 4 is advised?
			- if size 4 is frequent. All its subset of size 3 must be frequent. Hence Size 3 subsets must have been generated. Hence, all combos of size 4 will be generated from size 3 and no information is lost while discarding the size 4 generated by multiplying F2 X F2
		- C1 -> F1 -> C2 -> F2 -> C3 -> F3 -> ...
		- Important rule:
			- If we are trying to form size k, we have to merge two subset which have atleast k-2 items common
			- if we want to make size 3: we can merge {1,2} + {2,3}
	- ## Ordering of Itemset
		- TODO what is lexicographical order
		- Items are sorted in lexicographical order
	- Frequent Itemset mining needs more memory and can not be done in RAM
	- Association rule mining can be done with less memory and with RAM
		-
- # Lecture 3
- # Lecture 4
  collapsed:: true
	- [[Jan 10th, 2025]]
	- Earlier algo: BFS -> Apriori Algo
	- New algo: DFS -  Frequent Pattern Tree Algo
	- ![Frequent Pattern Mining.pdf](../assets/Frequent_Pattern_Mining_1736504057586_0.pdf)
	- Read ((6780f312-526d-4617-9ed2-e8a8e8718f38))
	- while adding {B, C, D} we link the different Bs as it will help in making the conditional FP trees
		- ((6780f596-09d2-4ec0-b50e-c914e4bbece2))
		- Tree can be loaded into memory even if the dataset is large as tree reduces the size
		- TODO What advantage will frequency based order  give?
		- |**Node**|**Freq**|
		  | -------- | ------- |
		  | A  | 7   |
		  | B | 8     |
		  | C    | 7    |
		  |D|5|
		  |E|3|
		- Start from lowest (least freq)->E and try to build all size 2, 3....
			- Conditional FP Tree
			- ((6780f9b6-12e4-472e-a73c-a449665e901d))
			- Header file helps in going to the leaves that contain E
			- In above B is deleted as B does not satisfy freq criteria (total sum of frequency is less than 2)
			- Freq AE:2, CE:2, DE:2
				- Use recursion to build conditional FP tree on D
					- ((6780fc9c-6a64-42ec-9fce-46184acf5c7e))
					- size 3-> ADE: 2
						- Further try recursion but it ends hence returns null
				- Use recursion to build conditional FP tree on C
					- A gets prunned
					- null
				- Use recursion to build conditional FP tree on A
					- null
			- Check with conditional FP on D
		- ### Benefit
			- Access of dataset is only two times in FP Tree Algo, no candidate generation
			- But in Apriori alg dataset access is multiple (k times for candidate of size k) times, no conditional FP tree
		- #Homework Proove that sorting using freq will help in shorter tree
		- #Homework Code FP tree and conditional FP via code
		- #Homework whether candidate generation is faster or conditional FP tree
- # Lecture 5
  collapsed:: true
	- [[Frequent SubGraph Mining]]
- # Lecture 6
  collapsed:: true
	- Isomorphic graphs -> Graph having similar structure
	- Size of graph -> number of edges in graph
- # Lecture 7
- # Lecture 8 [[Jan 28th, 2025]]
  collapsed:: true
	- Prev class:
		- Canonical labels
- # Lecture 12 [[Feb 14th, 2025]]
	- Limitations of K means
		- Convex shapes
		- How do you select k?
		- Non-homogenous cluster sizes
		- Susceptibility to outliers
	- Density based algorithms
		- Automatically determine k
		- Automatically handles outliers
	- DB scan
		- Density based scans - find out all high density based regions
		- inputs required -> maximum radius of neighborhood, minimum number of points in neighborhood