# Lecture 1
	- ![lec1.pdf](../assets/lec1_1736626507309_0.pdf)
	- NLP - Natural Language Processing
	- https://lcs2.in/nlp2402
	- https://web.stanford.edu/~jurafsky/slp3/
	- TA : Sahil Mishra, Aswini, Anwoy, Vaibhav
	- Libraries nltk, sk-learn, Spacy, Stanza, Shallow Parser (Indian Language), Universal Parser(Multi-lingual), CMU ARK, Stanford CoreNLP
	- Books
		- Speech Language Processing by Daniel Jurafsky
		- Introduction to large Language Models by Tanmoy Chakraborty
		-
	- {{video https://www.youtube.com/watch?v=808M7q8QX0E&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=1}}
	- regexpal.com, book link - https://web.stanford.edu/~jurafsky/slp3/ed3bookaug20_2024.pdf
	-
- # Lecture 2
	- ## Matter
		- Regular Expression
			- Range - [wW], [a-z], [A-Za-z]
			- Negation - [ ^Ss \] - neither S or s
				- [ ^ A-Z ] - neither A-Z
				- [e^] - look for either e or ^
				- a^b - look for a, ^, b
			- Optional - ? - Woodchucks?, Colou?rs? -> color, colors, colour, colours
			- OR - [a|b]
			- Multiple
				- t* -> 0 or more '',t, tt, ttt
				- b+ -> 1 or more
			- any character - . -> beg.n -> begin, began, begon ....
			- Finding all The - > (^| )[tT]he( |\.|$)
		- Accuracy or Precision -> Minimize false +ve
		- Recall  -> minimize false -ve
		- Morphology
			- TODO What is Morphology?
			- Two kind of knowledge
				- Orthographic rules - can solve woodchucks vs woodchuck
				- Morphological rules -  can solve goose vs geese
			- Why do we need to understand a language?
				- some language adds prefix or suffix to root word like english, hindi, latin
				- some languages such as tagalog add infix  or circumfix (german)
			- TODO Read about Automata Chapter 2 and 3 of Book
			- TODO Read about basic terminologies
			- Concatenative morphology is easy -  chapter 2 and 3
		- Stemming Methods
			- TODO Lemmatization, stemming, difference between these two?
		- Port Stemmer (1980)
			- 7 Sets of manual sequential rules
			- May not return valid stem word but guarantees if two word have same root they will be mapped similarly
			- All words are of the form (C)(VC)^m (V) ---- where C is consonent, V is vowel
				- consonent are letter other than A, E, I, O, U and Y preceeded by consonent
				- Trouble -> C(VC)^2
				- Apple -> (VC)^2
			- TODO Watch a video on how do the [[portstemmer]] work?
			-
- # Lecture 3
	- ## Matter
	- TODO do [[Levenshtein Distance]]
	- Statistical language Models
		- TODO [[N-Grams]]
		- Benefits:
			- Machine Translation
			- Spelling Correction
			- Speech Recognition
		- How Naive Bayes Formula is applied word by word in [[N-Grams]] ?
		- **Issues with larger context**
			- Data sparsity - larger matrix but mostly items will be zero and few combinations will be there in corpus
			- Issue of over-fitting -> It will give the sentences already available in corpus
			- High computational cost
			- **Limited contextual understanding** of long-range dependencies.
		- **Why do we use sampling?**
			- It is used to ensure **diversity**, **creativity**, and **control** in the generated outputs.
			- We use [[Temperature Sampling]] sometime to incorporate the control over the creativity and deterministic outcome
			- To avoid repetition
		- TODO Contingent Zero vs Structural Zero
			- A **contingent zero** refers to a situation where a **morpheme is absent** in a particular context but could potentially have a realization in other contexts.
			- A **structural zero** is a situation where a **morpheme is absent** because its absence is required by the grammatical structure or rules of the language.
		- Generalisation and zero
		- Shanon Visualisation
		- Shakespeare as a corpus
			- N = 884,647, V = 29,066, Possible biagrams = 9,00000000
			- But actually shakespeare had only 3,00,000 bigrams, 99.99% matrix is zeroes -> very very sparse
			-
	- ## Zero mitigation - Smoothing techniques
		- ### Adding Ones - One Smoothing
			- $=\frac{count\left(I\ am\right)+1}{Count\left(I\right)+\left|V\right|}$
			- $=\frac{count\left(I\ am\right)+\alpha}{Count\left(I\right)+\left|V\right|}$
	- ## Markov assumption
		- **First Order**
			- In a **first-order Markov process**, the future state depends only on the current state and not on any earlier states. This means that the process has **no memory** beyond the current state, which is the essence of the Markov property.
			- $P(X_{n+1}\mid X_{n},X_{n-1},\dots,X_0)=P(X_{n+1}\mid X_{n})$
		- **Second Order**
			- In a **second-order Markov process**, the future state depends on the two most recent states, i.e., the current state and the previous state.
			- $P(X_{n+1}\mid X_{n},X_{n-1})=P(X_{n+1}\mid X_{n},X_{n-1})$
			-
			-
		-
- # Lecture 4
-
-