# Lecture 1
	- ##
- # Lecture 2
	- ## Matter
		- Regular Expression
			- Range - [wW], [a-z], [A-Za-z]
			- Negation - [ ^Ss \] - neither S or s
				- [ ^ A-Z ] - neither A-Z
				- [e^] - look for either e or ^
				- a^b - look for a, ^, b
			- Optional - ? - Woodchucks?, Colou?rs? -> color, colors, colour, colours
			- OR - [a|b]
			- Multiple
				- t* -> 0 or more '',t, tt, ttt
				- b+ -> 1 or more
			- any character - . -> beg.n -> begin, began, begon ....
			- Finding all The - > (^| )[tT]he( |\.|$)
		- Accuracy or Precision -> Minimize false +ve
		- Recall  -> minimize false -ve
		- Morphology
			- TODO What is Morphology?
			- Two kind of knowledge
				- Orthographic rules - can solve woodchucks vs woodchuck
				- Morphological rules -  can solve goose vs geese
			- Why do we need to understand a language?
				- some language adds prefix or suffix to root word like english, hindi, latin
				- some languages such as tagalog add infix  or circumfix (german)
			- TODO Read about Automata Chapter 2 and 3 of Book
			- TODO Read about basic terminologies
			- Concatenative morphology is easy -  chapter 2 and 3
		- Stemming Methods
			- TODO Lemmatization, stemming, difference between these two?
		- Port Stemmer (1980)
			- 7 Sets of manual sequential rules
			- May not return valid stem word but guarantees if two word have same root they will be mapped similarly
			- All words are of the form (C)(VC)^m (V) ---- where C is consonent, V is vowel
				- consonent are letter other than A, E, I, O, U and Y preceeded by consonent
				- Trouble -> C(VC)^2
				- Apple -> (VC)^2
			- TODO Watch a video on how do the [[portstemmer]] work?
			-
- # Lecture 3
	- ## Matter
	- TODO do [[Levenshtein Distance]]
	- Statistical language Models
		- TODO [[N-Grams]]
		- Benefits:
			- Machine Translation
			- Spelling Correction
			- Speech Recognition
		- How Naive Bayes Formula is applied word by word in [[N-Grams]] ?
		- **Issues with larger context**
			- Data sparsity - larger matrix but mostly items will be zero and few combinations will be there in corpus
			- Issue of over-fitting -> It will give the sentences already available in corpus
			- High computational cost
			- **Limited contextual understanding** of long-range dependencies.
		- **Why do we use sampling?**
			- It is used to ensure **diversity**, **creativity**, and **control** in the generated outputs.
			- We use [[Temperature Sampling]] sometime to incorporate the control over the creativity and deterministic outcome
			- To avoid repetition
		- TODO Contingent Zero vs Structural Zero
		-
- # Lecture 4
-