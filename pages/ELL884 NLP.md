# Lecture 1
	- ![lec1.pdf](../assets/lec1_1736626507309_0.pdf)
	- NLP - Natural Language Processing
	- https://lcs2.in/nlp2402
	- https://web.stanford.edu/~jurafsky/slp3/
	- TA : Sahil Mishra, Aswini, Anwoy, Vaibhav
	- Libraries nltk, sk-learn, Spacy, Stanza, Shallow Parser (Indian Language), Universal Parser(Multi-lingual), CMU ARK, Stanford CoreNLP
	- Books
		- Speech Language Processing by Daniel Jurafsky
		- Introduction to large Language Models by Tanmoy Chakraborty
		-
	- {{video https://www.youtube.com/watch?v=808M7q8QX0E&list=PLaZQkZp6WhWyvdiP49JG-rjyTPck_hvEu&index=1}}
	- regexpal.com, book link - https://web.stanford.edu/~jurafsky/slp3/ed3bookaug20_2024.pdf
	- {{youtube-timestamp 16}}
	-
- # Lecture 2
	- ![lec2.pdf](../assets/lec2_1737997815813_0.pdf)
	- ((6797be1a-3b26-4991-952d-6dcb22bd0beb))
	- Type I error: False Positive
	- Type II error: False Negative
		- Accuracy or Precision:-> Minimising False Positive
		- Coverage or Recall -> Minimising False negative
	- Orthographic rules -> for breaking words in stem and modifier
	- Morphology-> Study of Word Structure. Can help in distinguishing goose and geese
	- ## Matter
		- Regular Expression
			- Range - [wW], [a-z], [A-Za-z]
			- Negation - [ ^Ss \] - neither S or s
				- [ ^ A-Z ] - neither A-Z
				- [e^] - look for either e or ^
				- a^b - look for a, ^, b
			- Optional - ? - Woodchucks?, Colou?rs? -> color, colors, colour, colours
			- OR - [a|b]
			- Multiple
				- t* -> 0 or more '',t, tt, ttt
				- b+ -> 1 or more
			- any character - . -> beg.n -> begin, began, begon ....
			- Finding all The - > (^| )[tT]he( |\.|$)
		- Accuracy or Precision -> Minimize false +ve
		- Recall  -> minimize false -ve
		- Morphology
			- TODO What is Morphology?
			- Two kind of knowledge
				- Orthographic rules - can solve woodchucks vs woodchuck
				- Morphological rules -  can solve goose vs geese
			- Why do we need to understand a language?
				- some language adds prefix or suffix to root word like english, hindi, latin
				- some languages such as tagalog add infix  or circumfix (german)
			- TODO Read about Automata Chapter 2 and 3 of Book
			- TODO Read about basic terminologies
			- **Concatenative morphology is easy** -  chapter 2 and 3
		- Stemming Methods
			- TODO Lemmatization, stemming, difference between these two?
		- Port Stemmer (1980)
			- 7 Sets of manual sequential rules
			- May not return valid stem word but guarantees if two word have same root they will be mapped similarly
			- All words are of the form (C)(VC)^m (V) ---- where C is consonent, V is vowel
				- consonent are letter other than A, E, I, O, U and Y preceeded by consonent
				- Trouble -> C(VC)^2
				- Apple -> (VC)^2
			- TODO Watch a video on how do the [[portstemmer]] work?
			-
- # Lecture 3
	- ![lec3a.pdf](../assets/lec3a_1737998208327_0.pdf)
	-
	- ## Matter
	- TODO do [[Levenshtein Distance]]
		- How similar are two strings?
		- Which is closest?
		- Uses:
			- Machine Translation
			- Information Extraction
			- Speech Recognition
			- Align two sequences of nucleotide
	- Statistical language Models
		- TODO [[N-Grams]]
		- Benefits:
			- Machine Translation
			- Spelling Correction
			- Speech Recognition
		- How Naive Bayes Formula is applied word by word in [[N-Grams]] ?
		- **Issues with larger context**
			- Data sparsity - larger matrix but mostly items will be zero and few combinations will be there in corpus
			- Issue of over-fitting -> It will give the sentences already available in corpus
			- High computational cost
			- **Limited contextual understanding** of long-range dependencies.
		- **Why do we use sampling?**
			- It is used to ensure **diversity**, **creativity**, and **control** in the generated outputs.
			- We use [[Temperature Sampling]] sometime to incorporate the control over the creativity and deterministic outcome
			- To avoid repetition
		- TODO Contingent Zero vs Structural Zero
			- A **contingent zero** refers to a situation where a **morpheme is absent** in a particular context but could potentially have a realization in other contexts.
			- A **structural zero** is a situation where a **morpheme is absent** because its absence is required by the grammatical structure or rules of the language.
		- Generalisation and zero
		- Shanon Visualisation
		- Shakespeare as a corpus
			- N = 884,647, V = 29,066, Possible biagrams = 9,00000000
			- But actually shakespeare had only 3,00,000 bigrams, 99.99% matrix is zeroes -> very very sparse
			-
	- ## Zero mitigation - Smoothing techniques
		- ### Adding Ones - One Smoothing
			- $=\frac{count\left(I\ am\right)+1}{Count\left(I\right)+\left|V\right|}$
			- $=\frac{count\left(I\ am\right)+\alpha}{Count\left(I\right)+\left|V\right|}$
	- ## Markov assumption
		- **First Order**
			- In a **first-order Markov process**, the future state depends only on the current state and not on any earlier states. This means that the process has **no memory** beyond the current state, which is the essence of the Markov property.
			- $P(X_{n+1}\mid X_{n},X_{n-1},\dots,X_0)=P(X_{n+1}\mid X_{n})$
		- **Second Order**
			- In a **second-order Markov process**, the future state depends on the two most recent states, i.e., the current state and the previous state.
			- $P(X_{n+1}\mid X_{n},X_{n-1})=P(X_{n+1}\mid X_{n},X_{n-1})$
			-
			-
		-
- # Lecture 4
-
	- {{video https://www.youtube.com/watch?v=tOMjTCO0htA}}
	- {{youtube-timestamp 24}} - 3 types of Subword tokenization
		- Byte pair encoding
		- Unigram language modeling
		- Word Piece
	- {{youtube-timestamp 442}} Morpheme is smallest meaning bearing unit of language
-
- {{video https://www.youtube.com/watch?v=bFfWbQoVmIA}}
	- Lemmatization reduces a word to its base or dictionary form (called the lemma). It considers the word's meaning and part of speech eg: *"better"* → *"good"* (semantic lemmatization)
	- Stemming reduces(crudly) a word to its base or root form by chopping off prefixes or suffixes without considering the word's context or grammatical correctness. eg *"flies"* → *"fli"*
	- {{youtube-timestamp 318}} Sentence segmentation. !,? are easy . is hard. hence first tokenization then sentences
- ## Lecture 5
	- Laplace-Smoothed bigrams
		- Bigrams which appeared lot of time -> their freq is decreased after smoothening
	- Backoff and interpolation
	- Stupid Back off
	- when some unigram etc are not in training but are their in test case
		- out of vocabulary -> unk token
		- method 1 -> replace lower frequency count with unk while training
			- then in test all OOV treated as per UNK probability
	- Advance Smoothing
		- Good TUring
			- keep the count of things we have never seen once
			- Frequency of frequency
		- Kneser ney
	- [[KLD]], [[Entropy]]