- **Definition** : A measure of uncertainty or randomness in a distribution
- Types:
	- High Entropy: Uniform Distribution
	- Low Entropy: Distribution concentrated around a few values (low uncertainty)
- **Cross Entropy**
	- measures how well a model q(x) predicts data distributed according to p(x)
	- used as loss function to train model
- $H(P,Q)=H(P)+D_{\text{KL}}(P\|Q)$
- Where:
	- H(P,Q) is the cross-entropy:
		- $H(P,Q)=-\int p(x)\log q(x)\,dx$
	- H(P) is the entropy of P :
		- $H(P)=-\int p(x)\log p(x)\,dx$
	- $D_{\text{KL}}(P\|Q)$ is the [[KLD]] :
		- $D_{\text{KL}}(P\|Q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx$
		-
- ## Explanation 2
	- Entropy is measure of surprise after set of events : entropy adds if probability multiplies
		- for rare events - probablity reduces, surprise increases..
		- Lowest Entropy/Surprise - Uniform distribution
		- Highest Entropy/Surprise - all data points near few points in space
	- Surprise of individual state - $\log\frac{1}{p_{s}(x)}$
	- Average surprise -> $\int p(x)\log\frac{1}{p(x)}\,dx$
	- ## Cross Entropy
		- If P and Q are same -> $H\left(P,Q\right)=H\left(P,P\right)=H\left(Q,Q\right)$