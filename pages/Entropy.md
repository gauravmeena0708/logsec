- **Definition** : A measure of uncertainty or randomness in a distribution
- Types:
	- High Entropy: Uniform Distribution
	- Low Entropy: Distribution concentrated around a few values (low uncertainty)
- **Cross Entropy**
	- measures how well a model q(x) predicts data distributed according to p(x)
	- used as loss function to train model
- $H(P,Q)=H(P)+D_{\text{KL}}(P\|Q)$
- Where:
	- H(P,Q) is the cross-entropy:
		- $H(P,Q)=-\int p(x)\log q(x)\,dx$
	- H(P) is the entropy of P :
		- $H(P)=-\int p(x)\log p(x)\,dx$
	- $D_{\text{KL}}(P\|Q)$ is the [[KLD]] :
		- $D_{\text{KL}}(P\|Q)=\int p(x)\log\frac{p(x)}{q(x)}\,dx$
	- ## Surprise
		- surprise = $\log\left(\frac{1}{P\left(x\right)}\right)$
- ## Explanation 2
	- Entropy is measure of surprise after set of events : entropy adds if probability multiplies
		- for rare events - probablity reduces, surprise increases..
		- Lowest Entropy/Surprise - Uniform distribution
		- Highest Entropy/Surprise - all data points near few points in space
	- Surprise of individual state - $\log\frac{1}{p_{s}(x)}$
	- Average surprise -> H(P,P) = $\int p(x)\log\frac{1}{p(x)}\,dx$
	- ## Cross Entropy
		- If P and Q are same -> $H\left(P,Q\right)=H\left(P,P\right)=H\left(Q,Q\right)$
		- If we use q to model p. the surprise is $\int p(x)\log\frac{1}{q(x)}\,dx$
		- To just measure the surprise caused by q we can write it as
			- [[KLD]] = H(P,Q) - H(P)
			- $\int p(x)\log\frac{1}{q(x)}\,dx$ - $\int p(x)\log\frac{1}{p(x)}\,dx$
			- or KLD = $\int p(x)\log\frac{p(x)}{q(x)}\,dx$
	- ## KLD
		- Hence, KLD is the true measure of models correctness
		- However, H(P,P) surprise inherent in actual Dist P is fixed
		- Hence, in most of the cases our optimization goal becomes minimization of cross entropy