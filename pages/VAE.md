- {{video https://www.youtube.com/watch?v=w8F7_rQZxXk&list=PLdxQ7SoCLQANizknbIiHzL_hYjEaI-wUe&index=1}}
	- Summary
		- Review of stacked Auto Encoders
		- Basics of Probability
		- KL Divergence and its significance
		- Derivation of loss function for VAE
		- Coding the VAE in Google Colab
	- Stacked Auto-Encoders
		- Image Reconstruction
		- {{youtube-timestamp 147}} Input -> Encoder -> Bottleneck -> Decoder -> Output
		- Reconstruction loss $\mathcal{L}_{rec}(\phi,\theta)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-f_{\theta}(g_{\phi}(x_{i})))^2$
		- Encoder $\mu=g_{\mu}(x;\phi),\quad\log\sigma^2=g_{\sigma}(x;\phi)$
		- Decoder $\hat{x}=f(z;\theta)$
		- Bottleneck/ Reparameterization certificate $z=\mu+\sigma\odot\epsilon,\quad\epsilon\sim\mathcal{N}(0,I)$
		- $\theta, \phi$ are weights and biases
		- Benefits:
			- Represent the original image in compressed size of bottleneck
			- reduces need of bandwidth in telecommunication channel
	- Denoising Autoencoder
		- Partially Destroyed input = input + Noise
		- Original input->partially destroyed input -> Encoder -> Bottleneck -> decoder -> Output
		- Loss $\mathcal{L}(\phi,\theta)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-f_{\theta}(g_{\phi}(\tilde{x}_{i})))^2$
	- Variational Encoder
		- Input -> **Probabilistic Encoder ($\mu,\sigma$)** -> Sampled Latent vector -> **Probabilistic Decoder** -> Ouput
		- Loss KL $\mathcal{L}_{KL}=\frac12\sum_{j=1}^{J}\left(\mu_{j}^2+\sigma_{j}^2-1-\log\sigma_{j}^2\right)$
		- Loss reconstruction $\mathcal{L}_{rec}(\phi,\theta)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-f_{\theta}(g_{\phi}(x_{i})))^2$
		- Total Loss = Loss kl + Loss Reconstruction
	- Prerequisite
		- [[KLD]]
		- P(x) - Probability
		- P(X|Y) - Conditional Probability
		- E[x] - Expectation
		- $P\left(Y\left|Y\right|\right)=\frac{P\left(X\left|Y\right|\right)P\left(Y\right)}{P\left(X\right)}$
		- Posterior = $\frac{\left(Likelihood\cdot Prior\right)}{Evidence}$ = $\frac{P\left(X,Y\right)}{P\left(X\right)}$
		-
		-
		-
		-
		-
		-
		-
		-
		-
	-
	-